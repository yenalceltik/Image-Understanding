{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from random import randint\n",
    "import math\n",
    "from numpy.linalg import inv\n",
    "\n",
    "def takeAvg(total,size):#To use in DLT\n",
    "    avg=total/float(size)\n",
    "    return avg\n",
    "def totalize(points,axis):#To use in DLT\n",
    "    total=0\n",
    "    for i in range(len(points)):\n",
    "        total+=float(points[i][axis])\n",
    "        \n",
    "    return total\n",
    "def normalize(points,avgx,avgy):#To use in DLT\n",
    "    newPoints=[]\n",
    "    for i in range(len(points)):\n",
    "        normalized_x=float(points[i][0])-avgx\n",
    "        normalized_y=float(points[i][1])-avgy\n",
    "        newPoints.append([normalized_x,normalized_y])\n",
    "        \n",
    "    return newPoints\n",
    "def scale(points):#To use in DLT\n",
    "    distance=0\n",
    "    for i in range(len(points)):\n",
    "        distance += float(np.sqrt(np.square(points[i][0]) + np.square(points[i][1])))\n",
    "    scale=  np.sqrt(2)/float(distance)\n",
    "    return scale\n",
    "def detect_keyPoints(img1,img2):\n",
    "    sift = cv2.xfeatures2d.SIFT_create(contrastThreshold = 0.1 ,edgeThreshold = 10)\n",
    "    kp1, des1 = sift.detectAndCompute(img1,None)\n",
    "    kp2, des2 = sift.detectAndCompute(img2,None)\n",
    "    newList1 = []\n",
    "    newList2 = []\n",
    "    temp=0\n",
    "    for i in range(len(des1)):\n",
    "        distance=1000000000\n",
    "\n",
    "        temp=0\n",
    "        for y in range(len(des2)):\n",
    "            x=0\n",
    "            for a in range(len(des1[0])):\n",
    "                x+=math.sqrt((des1[i][a]-des2[y][a])**2)\n",
    "\n",
    "            if(x<distance):\n",
    "                distance=x\n",
    "                temp=int(y)\n",
    "        newList1.append(kp1[i].pt)\n",
    "        newList2.append(kp2[temp].pt)\n",
    "        \n",
    "    return newList1,newList2\n",
    "\n",
    "def dlt(ref_points,dst_points):\n",
    "    \n",
    "    total_x=totalize(ref_points,0)\n",
    "    total_y=totalize(ref_points,1)\n",
    "    total_xprm=totalize(dst_points,0)\n",
    "    totaly_yprm=totalize(dst_points,1)\n",
    "    \n",
    "    avg_x=takeAvg(total_x,len(ref_points))\n",
    "    avg_y=takeAvg(total_y,len(ref_points))\n",
    "    avg_xprm=takeAvg(total_xprm,len(dst_points))\n",
    "    avg_yprm=takeAvg(totaly_yprm,len(dst_points))\n",
    "\n",
    "    new_refPoints=normalize(ref_points,avg_x,avg_y)\n",
    "    new_dstPoints=normalize(dst_points,avg_xprm,avg_yprm)\n",
    "    \n",
    "    ref_scale=scale(new_refPoints)\n",
    "    dst_scale=scale(new_dstPoints)\n",
    "    \n",
    "    t=np.array([[float(ref_scale),0,float(ref_scale)*(-avg_x)]\n",
    "                ,[0,float(ref_scale),float(ref_scale)*(-avg_y)],\n",
    "                 [0,0,1]])\n",
    "        \n",
    "    t2=np.array([[float(dst_scale),0,float(dst_scale)*(-avg_xprm)],\n",
    "                 [0,float(dst_scale),float(dst_scale)*(-avg_yprm)],\n",
    "                 [0,0,1]])\n",
    "    \n",
    "    for i in range(len(new_refPoints)):\n",
    "        nPoints=np.dot(t,np.array([[float(ref_points[i][0])],[float(ref_points[i][1])],[1]]))\n",
    "        new_refPoints[i][0]= nPoints[0]\n",
    "        new_refPoints[i][1]= nPoints[1]\n",
    "     \n",
    "    for i in range(len(new_dstPoints)):\n",
    "        nPoints2=np.dot(t2,np.array([[float(dst_points[i][0])],[float(dst_points[i][1])],[1]]))\n",
    "        new_dstPoints[i][0]= nPoints2[0]\n",
    "        new_dstPoints[i][1]= nPoints2[1]\n",
    "        \n",
    "    A_matrix=[]\n",
    "    \n",
    "    for i in range(len(new_refPoints)):\n",
    "        A_matrix.append([0,0,0,-new_refPoints[i][0],-new_refPoints[i][1],-1,new_dstPoints[i][1]*new_refPoints[i][0],new_dstPoints[i][1]*new_refPoints[i][1],new_dstPoints[i][1]])\n",
    "        A_matrix.append([new_refPoints[i][0],new_refPoints[i][1],1,0,0,0,-new_dstPoints[i][0]*new_refPoints[i][0],-new_refPoints[i][1]*new_dstPoints[i][0],-new_dstPoints[i][0]])\n",
    "\n",
    "    A_matrix=np.array(A_matrix)\n",
    "    w,u,vt=cv2.SVDecomp(np.asarray(A_matrix))\n",
    "    H̃=[[vt[-1][0],vt[-1][1],vt[-1][2]],[vt[-1][3],vt[-1][4],vt[-1][5]],[vt[-1][6],vt[-1][7],vt[-1][8]]]\n",
    "    invT = inv(t2)   \n",
    "    tempH=np.dot(invT,H̃)\n",
    "    H=np.dot(tempH,t)\n",
    "    return H\n",
    "\n",
    "\n",
    "def findInlier(H,keyPoints1,keyPoints2):\n",
    "    current_refPoints=[]\n",
    "    current_dstPoints=[]\n",
    "    temp=0\n",
    "    counter=0\n",
    "    for x in range(len(keyPoints1)):\n",
    "        distance=3.0\n",
    "        temp=0\n",
    "        y=0\n",
    "        queryList=np.dot(H,np.array([[keyPoints1[x][0]],[keyPoints1[x][1]],[1]]))\n",
    "        queryList = np.array(queryList)\n",
    "        queryList[0]=queryList[0]/queryList[2]\n",
    "        queryList[1]=queryList[1]/queryList[2]\n",
    "        flag = False\n",
    "        \n",
    "        for y in  range (len(keyPoints2)):\n",
    "            d=float(np.sqrt(np.square((queryList[0]-keyPoints2[y][0]))+(np.square(queryList[1]-keyPoints2[y][1]))))\n",
    "            if (d<=3.0):\n",
    "\n",
    "                if (distance>d):\n",
    "                    distance=d\n",
    "                    temp=int(y)\n",
    "                    flag = True \n",
    "\n",
    "        if (flag == True):\n",
    "            current_refPoints.append((keyPoints1[x]))\n",
    "            current_dstPoints.append((keyPoints2[temp]))\n",
    "            temp=0\n",
    "            \n",
    "    return current_refPoints,current_dstPoints\n",
    "\n",
    "img1 = cv2.imread('img1.jpg',0) \n",
    "img2 = cv2.imread('img2.jpg',0)\n",
    "keyPoint1,keyPoint2=detect_keyPoints(img1,img2)\n",
    "\n",
    "keyPoint1=np.array(keyPoint1)\n",
    "keyPoint2=np.array(keyPoint2)\n",
    "\n",
    "N=10000\n",
    "counter=0\n",
    "prev_numberOfinliers=0\n",
    "while(counter<N):\n",
    "    \n",
    "    numberOfinliers=0\n",
    "    random=np.random.randint(0,len(keyPoint1))\n",
    "    random2=np.random.randint(0,len(keyPoint1))\n",
    "    random3=np.random.randint(0,len(keyPoint1))\n",
    "    random4=np.random.randint(0,len(keyPoint1))\n",
    "    \n",
    "    refPoints=[(keyPoint1[random]),(keyPoint1[random2]),(keyPoint1[random3]),(keyPoint1[random4])]\n",
    "    dstPoints=[(keyPoint2[random]),(keyPoint2[random2]),(keyPoint2[random3]),(keyPoint2[random4])]\n",
    "  \n",
    "    refPoints=np.array(refPoints)\n",
    "    dstPoints=np.array(dstPoints)\n",
    "  \n",
    "    H=dlt(refPoints,dstPoints)\n",
    "    \n",
    "    for i in range(len(keyPoint1)):\n",
    "        queryPoints=np.dot(H,np.array([[keyPoint1[i][0]],[keyPoint1[i][1]],[1]]))\n",
    "    \n",
    "        queryPoints[0]=queryPoints[0]/queryPoints[2]\n",
    "        queryPoints[1]=queryPoints[1]/queryPoints[2]\n",
    "        queryPoints=np.array(queryPoints)\n",
    "        \n",
    "        for y in range (len(keyPoint2)):\n",
    "            \n",
    "            if (np.sqrt(np.square((queryPoints[0]-keyPoint2[y][0]))+(np.square(queryPoints[1]-keyPoint2[y][1])))<=3.0):\n",
    "                numberOfinliers+=1\n",
    "                break\n",
    "                \n",
    "    ratio=float(numberOfinliers)/len(keyPoint1)\n",
    "    \n",
    "    if(numberOfinliers>prev_numberOfinliers):\n",
    "            Homography=np.copy(H)\n",
    "            max_numOfinliers=numberOfinliers\n",
    "            N=int((-2.0/float(math.log10(1.0-(float(ratio**4))))) - 1.0)\n",
    "            counter=0\n",
    "            \n",
    "    counter+=1\n",
    "    prev_numberOfinliers=max_numOfinliers\n",
    "    print (\"RANSAC\",N)\n",
    "    print (\"Number of inlier is\",max_numOfinliers)\n",
    "\n",
    "print (\"INLIER RATIO:\",(max_numOfinliers/len(keyPoint1))*100)\n",
    "newRef=[]\n",
    "newDst=[]\n",
    "newRefCopy=[]\n",
    "newDstCopy=[]\n",
    "\n",
    "newRef,newDst=findInlier(Homography,keyPoint1,keyPoint2)\n",
    "\n",
    "n=0\n",
    "while(1):\n",
    "  \n",
    "    betterHomo=dlt(newRef,newDst)\n",
    "    n+=1\n",
    "    newRefCopy=np.copy(newRef)\n",
    "    newDstCopy=np.copy(newDst)\n",
    "\n",
    "    newRef=[]\n",
    "    newDst=[]\n",
    "    if (len(newRefCopy) <= 4 or  len(newDstCopy) <= 4):\n",
    "        break\n",
    "    newRef,newDst=findInlier(betterHomo,newRefCopy,newDstCopy)\n",
    "    \n",
    "    \n",
    "    if (len(newRef)==len(newRefCopy)):\n",
    "        break\n",
    "\n",
    "leftCoord=np.dot(betterHomo,np.array([[154.0],[161.0],[1.0]]))\n",
    "leftCoord[0]=float(leftCoord[0]/leftCoord[2])\n",
    "leftCoord[1]=float(leftCoord[1]/leftCoord[2])\n",
    "\n",
    "rightCoord=np.dot(betterHomo,np.array([[470.0],[161.0],[1.0]]))\n",
    "rightCoord[0]=(rightCoord[0]/rightCoord[2])\n",
    "rightCoord[1]=(rightCoord[1]/rightCoord[2])\n",
    "\n",
    "bottomleftCoord=np.dot(betterHomo,np.array([[172.0],[576.0],[1.0]]))\n",
    "bottomleftCoord[0]=(bottomleftCoord[0]/bottomleftCoord[2])\n",
    "bottomleftCoord[1]=(bottomleftCoord[1]/bottomleftCoord[2])\n",
    "\n",
    "bottomrightCoord=np.dot(betterHomo,np.array([[455.0],[576.0],[1.0]]))\n",
    "bottomrightCoord[0]=float(bottomrightCoord[0]/bottomrightCoord[2])\n",
    "bottomrightCoord[1]=float(bottomrightCoord[1]/bottomrightCoord[2])\n",
    "\n",
    "img4 = cv2.imread('img2.jpg')\n",
    "\n",
    "cv2.rectangle(img4,(leftCoord[0],leftCoord[1]),(bottomrightCoord[0],bottomrightCoord[1]),(0,0,255),5)\n",
    "cv2.imwrite(\"object_detection.png\",img4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMMENT-1:\n",
    "-ORB is a very fast binary descriptor based on BRIEF which is rotation invariant and resistant to noise. ORB is at two orders of magnitude faster than SIFT, while performing as well in many\n",
    "situations. The efficiency is tested on several real-world applications.\n",
    "-Scale-Invariant Feature  Transform  (SIFT)  that  has  similar  matching  performance,  is  less  affected  by  image  noise,  and  is  capable of  being  used  for  real-time  performance.\n",
    "-ORB  builds  on the FAST keypoint detector and  the  binary  BRIEF  descriptor with many modifications to enhance the performance. It uses FAST to find multiscale keypoints on several pyramid levels and applies a Harris corner measure to pick the best keypoints. To achieve rotation invariance, the orientation of the keypoint is calculated by using the intensity-weighted centroid of a circular patch with the\n",
    "located  keypoint  at  the  centre. \n",
    "\n",
    "NOTE:IN that comment i searched information in this site: http://www.willowgarage.com/sites/default/files/orb_final.pdf\n",
    "\n",
    "COMMENT-2:\n",
    "-We can find the nearest neighbour with calculating distance of each keypoint of reference image with all keypoints of the query image and then we can apply an algorithm to find minimum of them and take it our nearest neighbour.\n",
    "-For SIFT alorithm we calculate Euclidean distance\n",
    "-For ORB alorithm we calculate Hamming distance\n",
    "\n",
    "COMMENT-3:\n",
    "-The input of the RANSAC algorithm is a set of data points which contains outliers. The goal is to find a model describing inliers from the given data set.\n",
    "-We give a set of points as an input to RANSAC algorithm it contains some outliers and some inliers.Our purpose on this algorithm is to find inliers.So setting the Euclidean distance smaller than 3 pixels \n",
    "is good because greather outliers will be eliminated and this produce good result.\n",
    "\n",
    "COMMENT-4:\n",
    "-When we normalize points we  puts both points on the border which makes the solution more stable and puts it further from the center.So it provide us simplicity about estimating. By normalizing the data set, we center the data and give it unit variance. These conditions are better handled.This normalization affects to result of SVD computation.When we normalize we get better ratios between values.And it decrease noise to get exact solution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
